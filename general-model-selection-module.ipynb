{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-09T07:43:07.146282Z","iopub.execute_input":"2023-11-09T07:43:07.146683Z","iopub.status.idle":"2023-11-09T07:43:08.221650Z","shell.execute_reply.started":"2023-11-09T07:43:07.146649Z","shell.execute_reply":"2023-11-09T07:43:08.220444Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/telecom-churn/telecom_churn.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2023-11-09T07:43:08.223673Z","iopub.execute_input":"2023-11-09T07:43:08.224115Z","iopub.status.idle":"2023-11-09T07:43:08.296785Z","shell.execute_reply.started":"2023-11-09T07:43:08.224084Z","shell.execute_reply":"2023-11-09T07:43:08.295476Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"      Churn  AccountWeeks  ContractRenewal  DataPlan  DataUsage  \\\n0         0           128                1         1       2.70   \n1         0           107                1         1       3.70   \n2         0           137                1         0       0.00   \n3         0            84                0         0       0.00   \n4         0            75                0         0       0.00   \n...     ...           ...              ...       ...        ...   \n3328      0           192                1         1       2.67   \n3329      0            68                1         0       0.34   \n3330      0            28                1         0       0.00   \n3331      0           184                0         0       0.00   \n3332      0            74                1         1       3.70   \n\n      CustServCalls  DayMins  DayCalls  MonthlyCharge  OverageFee  RoamMins  \n0                 1    265.1       110           89.0        9.87      10.0  \n1                 1    161.6       123           82.0        9.78      13.7  \n2                 0    243.4       114           52.0        6.06      12.2  \n3                 2    299.4        71           57.0        3.10       6.6  \n4                 3    166.7       113           41.0        7.42      10.1  \n...             ...      ...       ...            ...         ...       ...  \n3328              2    156.2        77           71.7       10.78       9.9  \n3329              3    231.1        57           56.4        7.67       9.6  \n3330              2    180.8       109           56.0       14.44      14.1  \n3331              2    213.8       105           50.0        7.98       5.0  \n3332              0    234.4       113          100.0       13.30      13.7  \n\n[3333 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Churn</th>\n      <th>AccountWeeks</th>\n      <th>ContractRenewal</th>\n      <th>DataPlan</th>\n      <th>DataUsage</th>\n      <th>CustServCalls</th>\n      <th>DayMins</th>\n      <th>DayCalls</th>\n      <th>MonthlyCharge</th>\n      <th>OverageFee</th>\n      <th>RoamMins</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>128</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2.70</td>\n      <td>1</td>\n      <td>265.1</td>\n      <td>110</td>\n      <td>89.0</td>\n      <td>9.87</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>107</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3.70</td>\n      <td>1</td>\n      <td>161.6</td>\n      <td>123</td>\n      <td>82.0</td>\n      <td>9.78</td>\n      <td>13.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>137</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>243.4</td>\n      <td>114</td>\n      <td>52.0</td>\n      <td>6.06</td>\n      <td>12.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>84</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>299.4</td>\n      <td>71</td>\n      <td>57.0</td>\n      <td>3.10</td>\n      <td>6.6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>75</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>3</td>\n      <td>166.7</td>\n      <td>113</td>\n      <td>41.0</td>\n      <td>7.42</td>\n      <td>10.1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3328</th>\n      <td>0</td>\n      <td>192</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2.67</td>\n      <td>2</td>\n      <td>156.2</td>\n      <td>77</td>\n      <td>71.7</td>\n      <td>10.78</td>\n      <td>9.9</td>\n    </tr>\n    <tr>\n      <th>3329</th>\n      <td>0</td>\n      <td>68</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.34</td>\n      <td>3</td>\n      <td>231.1</td>\n      <td>57</td>\n      <td>56.4</td>\n      <td>7.67</td>\n      <td>9.6</td>\n    </tr>\n    <tr>\n      <th>3330</th>\n      <td>0</td>\n      <td>28</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>180.8</td>\n      <td>109</td>\n      <td>56.0</td>\n      <td>14.44</td>\n      <td>14.1</td>\n    </tr>\n    <tr>\n      <th>3331</th>\n      <td>0</td>\n      <td>184</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>213.8</td>\n      <td>105</td>\n      <td>50.0</td>\n      <td>7.98</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>3332</th>\n      <td>0</td>\n      <td>74</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3.70</td>\n      <td>0</td>\n      <td>234.4</td>\n      <td>113</td>\n      <td>100.0</td>\n      <td>13.30</td>\n      <td>13.7</td>\n    </tr>\n  </tbody>\n</table>\n<p>3333 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = data.copy()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T07:43:08.297931Z","iopub.execute_input":"2023-11-09T07:43:08.298231Z","iopub.status.idle":"2023-11-09T07:43:08.303094Z","shell.execute_reply.started":"2023-11-09T07:43:08.298206Z","shell.execute_reply":"2023-11-09T07:43:08.302043Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"X = df.drop(columns=['Churn'])\ny = df['Churn']","metadata":{"execution":{"iopub.status.busy":"2023-11-09T07:43:08.305210Z","iopub.execute_input":"2023-11-09T07:43:08.305789Z","iopub.status.idle":"2023-11-09T07:43:08.320480Z","shell.execute_reply.started":"2023-11-09T07:43:08.305752Z","shell.execute_reply":"2023-11-09T07:43:08.319352Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T07:43:08.404769Z","iopub.execute_input":"2023-11-09T07:43:08.405936Z","iopub.status.idle":"2023-11-09T07:43:08.414073Z","shell.execute_reply.started":"2023-11-09T07:43:08.405891Z","shell.execute_reply":"2023-11-09T07:43:08.412932Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"___\n# Build a model selection module","metadata":{}},{"cell_type":"code","source":"# Classification models\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-11-09T07:43:11.184065Z","iopub.execute_input":"2023-11-09T07:43:11.184576Z","iopub.status.idle":"2023-11-09T07:43:12.751367Z","shell.execute_reply.started":"2023-11-09T07:43:11.184535Z","shell.execute_reply":"2023-11-09T07:43:12.750218Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"```python\n>> selecion_pipe = GMSModule(mode='binary_classification',\nmetrics=['f1-score', 'accuracy', 'precision'],\ninclude=[LinearRegression(),\nRidgeRegression()...]),\ndata=[X_train, X_test, y_train, y_test],\nverbose=True)\n\n>> selection_pipe.run()\n>> selection_pipe.name() # LGBMCLassifier\n>> selection_pipe.describe() # Linear Regression - 0.94...\n>> selection_pipe.evaluation() # F1-score: 0.94\n                               # Accuracy: 0.96...\n```","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2023-11-09T07:43:16.037868Z","iopub.execute_input":"2023-11-09T07:43:16.038304Z","iopub.status.idle":"2023-11-09T07:43:16.043568Z","shell.execute_reply.started":"2023-11-09T07:43:16.038269Z","shell.execute_reply":"2023-11-09T07:43:16.042007Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Define a custom scoring function\ndef neg_mean_absolute_error(y_true, y_pred):\n    return -mean_absolute_error(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T07:43:16.269825Z","iopub.execute_input":"2023-11-09T07:43:16.270587Z","iopub.status.idle":"2023-11-09T07:43:16.275859Z","shell.execute_reply.started":"2023-11-09T07:43:16.270551Z","shell.execute_reply":"2023-11-09T07:43:16.274799Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import math\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\nfrom tabulate import tabulate\nfrom tqdm import tqdm\nimport pandas as pd\n                            \n\nclass GMSModule:\n    def __init__(self, *, mode: str, include: list, data: list, metrics: list, pivot: str = None) -> None:\n        \"\"\"\n        Initiate variables for work / catch errors created by users while creating an object\n        \n        Args:\n            - mode: A 'string' argument, that tells program which task is user solving (see 'valid_modes' for valid values)\n                e.g.: mode='classification'\n                \n            - include: A 'list' arg. that contains a list of models/pipelines\n                e.g.: include=[LinearRegression()]\n                \n            - data: A 'list' arg. that contains training and validation subsets\n                e.g.: data=[X_train, X_test, y_train, y_val]\n                \n            - metrics: A 'list' arg. that contains strings of names of metrics (see 'valid_*_metrics' for valid values)\n                e.g.: metrics=['accuracy', 'f1-score']\n                \n            - pivot: A 'str' argument that contains the name of the metric that the user considers important for solving their task\n            (if pivot=None -> the sum of the metrics would be the judge of the best model to consider)\n                e.g.: pivot='f1-score'\n                \n        Returns:\n            Nothing.\n        \"\"\"\n\n        self.mode = mode\n        self.metrics = metrics\n        self.include = include\n        self.pivot = pivot\n        self.X_train, self.X_test, self.y_train, self.y_test = data\n        \n        \n        # Eval. functions to consider\n        self.__evaluation_functions = {\n            # For classification\n            'accuracy': accuracy_score,\n            'precision': lambda y_true, y_pred: precision_score(y_true, y_pred, average='weighted'),\n            'recall': lambda y_true, y_pred: recall_score(y_true, y_pred, average='weighted'),\n            'f1-score': lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted'),\n            'roc-auc': lambda y_true, y_pred: roc_auc_score(y_true, y_pred, multi_class='ovr'),\n            \n            \n            # For regression\n            'mae': mean_absolute_error,\n            'mape': mean_absolute_percentage_error,\n            'mse': mean_squared_error,\n            'rmse': lambda y_true, y_pred: math.sqrt(mean_squared_error(y_true, y_pred)),\n            'r2-score': r2_score\n        }\n        \n        \n        \n        # Check for evaluation results (to avoid uneeded re-evaluation)\n        self.model_results = None\n        \n        \n        valid_modes = ['classification', 'regression']\n        valid_data_types = ['list', 'tuple']\n        valid_pivot_vals = list(self.__evaluation_functions.keys()) + [None]\n        \n        \n        # Catch 'mode' error\n        if self.mode not in valid_modes:\n            raise ValueError(f\"'{self.mode}' mode is not valid. Valid modes are in: {valid_modes}\")\n            \n        # Catch 'metrics' error (for metrics, that are strictly added by author)\n        if any(metric not in self.__evaluation_functions.keys() for metric in self.metrics):\n            raise ValueError(f\"{self.metrics} | One or more classification metrics are not valid. Valid metrics for classification are in: {self.evaluation_functions.keys()}\")\n            \n        # Catch 'pivot' error\n        if self.pivot not in valid_pivot_vals:\n            raise ValueError(f\"'{self.pivot}' name is not valid. Valid pivot names are in: {valid_pivot_vals}\")\n            \n        # Catch 'pivot is not in metrics' error\n        if self.pivot not in self.metrics + [None]:\n            raise ValueError(f\"'{self.pivot}' is not in given metrics: {self.metrics}\")\n            \n\n            \n    def _evaluate_models(self) -> (list, list):\n        \"\"\"\n        The Backbone of this module. Evaluate each model on different subsets using metrics given by the user\n        \n        Args:\n            - self: all self.variables from __init__ \n                \n        Returns:\n            A 'list' of 'tuple' values. Each tuple contains (*model_name, *dict_of_each_metric_evaluation). For both subsets: train and test\n        \"\"\"\n        \n        # Init. dicts to inflate with model_names as keys and predictions as values\n        self.model_predictions_dict_test = dict()\n        self.model_predictions_dict_train = dict()\n\n        \n        \n        # Check if evaluation was already done\n        if self.model_results is not None:\n            return self.model_results\n\n        else:\n            results_test = []\n            results_train = []\n\n            for model in tqdm(self.include, desc=\"Evaluating Each Model\", unit=\" eval.\"):\n                model = model.fit(self.X_train, self.y_train)\n                \n                \n                y_pred_test = model.predict(self.X_test)\n                y_pred_train = model.predict(self.X_train)\n                \n                self.model_predictions_dict_test[str(model)] = y_pred_test\n                self.model_predictions_dict_train[str(model)] = y_pred_train\n\n                \n                # Keep keys as metric names and evals as values\n                scores_test = {}\n                scores_train = {}\n                    \n                # Evaluate each model on each metric given\n                for metric in self.metrics:\n                    evaluation_function = self.__evaluation_functions.get(metric)\n                    \n                    scores_test[metric] = evaluation_function(self.y_test, y_pred_test)\n                    scores_train[metric] = evaluation_function(self.y_train, y_pred_train)\n                \n                \n\n                results_test.append((model, scores_test))\n                results_train.append((model, scores_train))\n\n            self.model_results = (results_train, results_test)\n            \n            return results_train, results_test\n            \n            \n            \n            \n\n    def best_model(self, print_info: bool = False) -> (str, dict):\n        \"\"\"\n        Get name and scores of the best model accroding to sum of the scores\n        \n        Args:\n            - self: variables from __init__ function\n                \n            - print_info: A 'bool' variable that tells function to print info or to return it for further usage\n                \n        Returns:\n            - if print_info=True: Nothing. Prints model name and score based on pivot\n            - else: Best model as object[0] / Predictions on test dataset[1]\n        \"\"\"\n        # Make scores global to output scores to user\n        global model_scores_dict_test\n        \n        result_train, result_test = self._evaluate_models()\n        \n\n        model_scores_dict_train = {}\n        model_scores_dict_test = {}\n\n        \n        # If self.pivot == None -> calculate sum of all scores\n        if self.pivot == None: \n            for model, scores in result_train:\n                score_sum = sum(scores.values())\n                model_scores_dict_train[model] = score_sum\n\n            # Iterate through the results and calculate the sum of scores | TEST\n            for model, scores in result_test:\n                score_sum = sum(scores.values())\n                model_scores_dict_test[model] = score_sum\n        \n        # If self.pivot != None -> get only self.pivot values\n        else:\n            for model, scores in result_train:\n                pivot_score = scores[self.pivot]\n                model_scores_dict_train[model] = pivot_score\n\n            # Iterate through the results and calculate the sum of scores | TEST\n            for model, scores in result_test:\n                pivot_score = scores[self.pivot]\n                model_scores_dict_test[model] = pivot_score\n\n        # Find the key with the maximum value in the dictionary\n        best_model = max(model_scores_dict_test, key=model_scores_dict_test.get)\n\n        # Print the key of the model with the highest score\n        if print_info:\n            print(f\"Model with the highest score: {best_model} \\nWith scores: {model_scores_dict_test[best_model]}\")\n        else:\n            best_model_preds = best_model.predict(self.X_test)\n            return best_model, best_model_preds\n        \n        \n        \n        \n    def create_ranking(self) -> dict:\n        \"\"\"\n        Receive a dictionary of each model ranked by its pivot\n        \n        Args:\n            - self: variables from __init__ function\n                \n        Returns:\n            'dict' that consists of \"Keys: 1-len(self.include), Values: models\"\n        \"\"\"\n        _, model_results = self._evaluate_models()\n\n        # Create a ranking based on the sum of values in the dictionaries\n        ranking = {}\n        \n        # Rank each model by its pivot metric\n        if self.pivot is not None:\n            for idx, (model, scores) in enumerate(\n                    sorted(model_results, key=lambda x: x[1][self.pivot], reverse=True) ):\n                ranking[idx + 1] = model\n        \n        # Rank each metric based on its sum of scores\n        else:\n            for idx, (model, scores) in enumerate(\n                    sorted(model_results, key=lambda x: -sum(score for score in x[1].values())) ):\n                ranking[idx + 1] = model\n\n        return ranking\n    \n    \n    \n    \n    def to_df(self, subset: str = \"test\") -> pd.DataFrame:\n        \"\"\"\n        Get a Dataframe of subset scores\n        \n        Args:\n            - subset: A string of two possible subsets: \"train\" or \"test\"\n            \n        Returns:\n            'pd.Dataframe' with model name and scores provided by user\n        \"\"\"\n        if subset not in ['train', 'test']:\n            raise ValueError(f\"Provided subset: '{subset}' is not in possible subsets: ['train', 'test']\")\n        \n        \n        results_train, results_test = self._evaluate_models()\n    \n        \n        # Model Names would always be as a column \n        columns = ['Model Name']\n        \n        \n        # For each metric provided create a column with its name\n        for i in self.metrics:\n            columns.append(i.title())\n            \n        data = []\n        \n        if subset == \"test\":\n            for i in range(len(self.include)):\n                # take its score predictions and create a list of [model_name, *scores] \n                temp_data = [str(self.include[i])] + list(results_test[i][1].values())\n                data.append(temp_data)\n                \n        elif subset == \"train\":\n            # the same as above ...\n            for i in range(len(self.include)):\n                temp_data = [str(self.include[i])] + list(results_train[i][1].values())\n                data.append(temp_data)\n            \n            \n        \n        return pd.DataFrame(data=data, columns=columns)\n    \n    \n    \n    \n    def get_predictions(self, model, subset: str = \"test\") -> list:\n        \"\"\"\n        Get predictions of a wanted model from the list of models provided.\n        \n        Args:\n            - model: An object of a model to get predictions from.\n            - subset: A string of a name of subset to get predictions from.\n                (default = \"test\")\n                \n        Returns:\n            List of predictions made by a model provided.\n        \"\"\"\n        # Receive predictions\n        self._evaluate_models()\n        \n        # Check if model given is not evaluated by Module\n        if str(model) not in list(map(str, self.include)):\n            raise ValueError(f\"Model name provided: '{model}' is not in evaluated models list: {self.include} \")\n        \n\n        if subset not in ['test', 'train']:\n            raise ValueError(f\"Subset provided: '{subset}' is not 'train' or 'test'\")\n            \n\n        if subset == \"train\":\n            return list(self.model_predictions_dict_train[str(model)])\n        \n        elif subset == \"test\":\n            return list(self.model_predictions_dict_test[str(model)])\n        \n        \n        \n        \n      \n\n        \n    def describe(self) -> None:\n        \"\"\"\n        Print (log) verbose information about each variable passed into the object and all possible returns.\n        Needed for user's self analysis and visual control\n        \n        Args:\n            - self: variables from __init__ function\n                \n        Returns:\n            Nothing. Prints description\n        \"\"\"\n        result_train, result_test = self._evaluate_models()\n        best_model_name, _ = self.best_model()\n        ranking = self.create_ranking()\n        \n        \n        print(\"==== DESCRIPTION ====\\n\")\n        \n        # Show inputed info\n        print(f\"Evaluation mode: {self.mode}\\n\")\n        print(f\"From models selected: {self.include}\\n\")\n        print(f\"Pivot selected: {self.pivot}\\n\" if self.pivot != None else f\"Pivot is not selected. Counting sums of scores!: pivot: {self.pivot}\\n\")\n        print(f\"Metrics for evaluation: {self.metrics}\\n\\n\")\n        \n        # Tables of evaluations for each model\n        print(\"# 1. Evaluation metrics for each model passed into the object for | TRAIN / TEST:\\n\")\n        print(tabulate(result_train, headers=[\"Models (train)\", \"Metrics (train)\"]), \"\\n\\n\")\n        print(tabulate(result_test, headers=[\"Models (test)\", \"Metrics (test)\"]), \"\\n\\n\")\n        \n        # Name and scores of the best model\n        print(\"# 2. Best model name and scores:\\n\")\n        print(f\"Best model name: {best_model_name}\\nBest model score: {max(model_scores_dict_test.values())}\\n\\n\")\n        \n        # Ranking of each model based on its pivot\n        print(\"# 3. Ranking of each model:\\n\")\n        for rank, model in ranking.items():\n            print(f\"{rank}: {model}\")\n            \n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-09T07:59:21.513791Z","iopub.execute_input":"2023-11-09T07:59:21.514211Z","iopub.status.idle":"2023-11-09T07:59:21.553403Z","shell.execute_reply.started":"2023-11-09T07:59:21.514177Z","shell.execute_reply":"2023-11-09T07:59:21.552100Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"GMSPipe = GMSModule(pivot=None, mode=\"classification\", metrics=['accuracy', 'f1-score'],\n                    include=[LogisticRegression(), RandomForestClassifier()],\n                    data=[X_train, X_test, y_train, y_test])\n\nGMSPipe.get_predictions(model=LogisticRegression(), subset='test')","metadata":{"execution":{"iopub.status.busy":"2023-11-09T07:59:21.555350Z","iopub.execute_input":"2023-11-09T07:59:21.555871Z","iopub.status.idle":"2023-11-09T07:59:22.304477Z","shell.execute_reply.started":"2023-11-09T07:59:21.555834Z","shell.execute_reply":"2023-11-09T07:59:22.302824Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"Evaluating Each Model: 100%|██████████| 2/2 [00:00<00:00,  2.78 eval./s]\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"[1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0]"},"metadata":{}}]},{"cell_type":"markdown","source":"```python\n[(LogisticRegression(),\n  {'accuracy': 0.8609112709832134, 'f1-score': 0.2564102564102564}),\n (RandomForestClassifier(),\n  {'accuracy': 0.9388489208633094, 'f1-score': 0.7605633802816901})]\n```","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
